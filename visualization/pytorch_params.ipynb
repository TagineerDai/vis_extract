{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Freezing related methods:  \n",
    "\n",
    "This is a discussion on freezing BatchNorm:  \n",
    "<https://discuss.pytorch.org/t/how-to-train-with-frozen-batchnorm/12106>\n",
    "\n",
    "This is a good module blocks version of Pytorch graph define:  \n",
    "<https://discuss.pytorch.org/t/train-nn-by-freezing-last-n-layers/9432/3>\n",
    "\n",
    "\n",
    "### 1. Use parameter group.  \n",
    "\n",
    "This is the [official given sample](https://gist.github.com/L0SG/2f6d81e4ad119c4f798ab81fa8d62d3f). \n",
    "\n",
    "```python\n",
    "# let's unfreeze the fc2 layer this time for extra tuning\n",
    "net.fc2.weight.requires_grad = True\n",
    "net.fc2.bias.requires_grad = True\n",
    "\n",
    "# add the unfrozen fc2 weight to the current optimizer\n",
    "optimizer.add_param_group({'params': net.fc2.parameters()})\n",
    "```\n",
    "\n",
    "### 2. Simply use require_grade by default inputs and reset the requires_grad flag.\n",
    "\n",
    "**Take care of the spreading mechasim**\n",
    "```python\n",
    "x = torch.randn(5, 5)  # requires_grad=False by default\n",
    "y = torch.randn(5, 5)  # requires_grad=False by default\n",
    "z = torch.randn((5, 5), requires_grad=True)\n",
    "a = x + y\n",
    "a.requires_grad # False\n",
    "b = a + z\n",
    "b.requires_grad # True\n",
    "```\n",
    "\n",
    "**A straight-forward example.**\n",
    "```python\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "# Replace the last fully-connected layer\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "model.fc = nn.Linear(512, 100)\n",
    "\n",
    "# Optimize only the classifier\n",
    "optimizer = optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9)\n",
    "```\n",
    "\n",
    "### 3. Use the filter on parameters().  \n",
    "\n",
    "```python\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=0.1)\n",
    "```\n",
    "Note that the above snippet assumed a common “train => save => load => freeze parts” scenario.\n",
    "\n",
    "### 4. Use children() interface.\n",
    "```python\n",
    "model_ft = models.resnet50(pretrained=True)\n",
    "ct = 0\n",
    "for child in model_ft.children():\n",
    "ct += 1\n",
    "if ct < 7:\n",
    "    for param in child.parameters():\n",
    "        param.requires_grad = False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "current_model = torchvision.models.resnet101(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "\n",
      "ReLU(inplace)\n",
      "\n",
      "MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "\n",
      "Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "\n",
      "Linear(in_features=512, out_features=7, bias=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List all components' __repr__.\n",
    "for child in current_model.children():\n",
    "    print(child)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1\n",
      "|-weight requries_grad is True.\n",
      "|-bias is not exist(NoneType).\n",
      "bn1\n",
      "|-weight requries_grad is True.\n",
      "|-bias requries_grad is True.\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "    0\n",
      "        conv1\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias is not exist(NoneType).\n",
      "        bn1\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias requries_grad is True.\n",
      "        relu\n",
      "        conv2\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias is not exist(NoneType).\n",
      "        bn2\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias requries_grad is True.\n",
      "    1\n",
      "        conv1\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias is not exist(NoneType).\n",
      "        bn1\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias requries_grad is True.\n",
      "        relu\n",
      "        conv2\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias is not exist(NoneType).\n",
      "        bn2\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias requries_grad is True.\n",
      "layer2\n",
      "    0\n",
      "        conv1\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias is not exist(NoneType).\n",
      "        bn1\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias requries_grad is True.\n",
      "        relu\n",
      "        conv2\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias is not exist(NoneType).\n",
      "        bn2\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias requries_grad is True.\n",
      "        downsample\n",
      "            0\n",
      "            |-weight requries_grad is True.\n",
      "            |-bias is not exist(NoneType).\n",
      "            1\n",
      "            |-weight requries_grad is True.\n",
      "            |-bias requries_grad is True.\n",
      "    1\n",
      "        conv1\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias is not exist(NoneType).\n",
      "        bn1\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias requries_grad is True.\n",
      "        relu\n",
      "        conv2\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias is not exist(NoneType).\n",
      "        bn2\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias requries_grad is True.\n",
      "layer3\n",
      "    0\n",
      "        conv1\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias is not exist(NoneType).\n",
      "        bn1\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias requries_grad is True.\n",
      "        relu\n",
      "        conv2\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias is not exist(NoneType).\n",
      "        bn2\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias requries_grad is True.\n",
      "        downsample\n",
      "            0\n",
      "            |-weight requries_grad is True.\n",
      "            |-bias is not exist(NoneType).\n",
      "            1\n",
      "            |-weight requries_grad is True.\n",
      "            |-bias requries_grad is True.\n",
      "    1\n",
      "        conv1\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias is not exist(NoneType).\n",
      "        bn1\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias requries_grad is True.\n",
      "        relu\n",
      "        conv2\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias is not exist(NoneType).\n",
      "        bn2\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias requries_grad is True.\n",
      "layer4\n",
      "    0\n",
      "        conv1\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias is not exist(NoneType).\n",
      "        bn1\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias requries_grad is True.\n",
      "        relu\n",
      "        conv2\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias is not exist(NoneType).\n",
      "        bn2\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias requries_grad is True.\n",
      "        downsample\n",
      "            0\n",
      "            |-weight requries_grad is True.\n",
      "            |-bias is not exist(NoneType).\n",
      "            1\n",
      "            |-weight requries_grad is True.\n",
      "            |-bias requries_grad is True.\n",
      "    1\n",
      "        conv1\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias is not exist(NoneType).\n",
      "        bn1\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias requries_grad is True.\n",
      "        relu\n",
      "        conv2\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias is not exist(NoneType).\n",
      "        bn2\n",
      "        |-weight requries_grad is True.\n",
      "        |-bias requries_grad is True.\n",
      "avgpool\n",
      "fc\n",
      "|-weight requries_grad is True.\n",
      "|-bias requries_grad is True.\n"
     ]
    }
   ],
   "source": [
    "def parse(current_model, depth=0):\n",
    "    'This module will list all the parameters recursively.'\n",
    "    current_depth = ''.join(['    ' for i in range(depth) ]) if depth!=0 else ''\n",
    "    for module_name in current_model._modules:\n",
    "        print(current_depth + module_name)\n",
    "        current_module = current_model._modules[module_name]\n",
    "        parameter_list = list(current_module._parameters.keys())\n",
    "        if not parameter_list:\n",
    "            parse(current_module, depth+1)\n",
    "        else:\n",
    "            for param_name in parameter_list:\n",
    "                try:\n",
    "                    print(current_depth + '|-' + param_name, 'requries_grad is', str(current_module._parameters[param_name].requires_grad)+'.')\n",
    "                except:\n",
    "                    print(current_depth + '|-' + param_name, 'is not exist(NoneType).')\n",
    "parse(current_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
